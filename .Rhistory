tmp$account_created_at <-df$created_at.y
tmp.2 <- list()
for (i in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[1]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.2[[i]] <- data.frame(profile_url)
}
tmp.2 <- do.call(rbind.data.frame, tmp.2)
tmp$profile_url <- tmp.2
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
tmp
})
df_tweets <- do.call(rbind.data.frame, tweets_list_clean)
colnames(df_tweets)[colnames(df_tweets) == 'id'] <- 'status_id'
colnames(df_tweets)[colnames(df_tweets) == "author_id"] <- 'user_id'
colnames(df_tweets)[colnames(df_tweets) == "following_count"] <- 'friends_count'
colnames(df_tweets)[colnames(df_tweets) == "tweet_count"] <- 'statuses_count'
geom <- list()
for(i in 1:nrow(df_tweets)){
if(length(df_tweets$geo_coords[[i]]) < 1){
df_tweets$geo_coords[[i]] <- c(NA, NA)
}
geom[[i]]<- c(df_tweets$geo_coords[[i]][[2]], df_tweets$geo_coords[[i]][[1]] )
}
df_tweets <-
df_tweets %>%
mutate(geo_coords = geom)
col_order <- c("user_id", "status_id", "created_at","screen_name", "text",
"source", "conversation_id", "in_reply_to_user_id",
"mentions_screen_name","retweet_count","reply_count", "like_count",
"quote_count","hashtags","url_t.co", "url_extended_url","lang",
"geo_coords","context_annotations","name","location", "description",
"followers_count","friends_count", "statuses_count",
"listed_count", "account_created_at","verified",
"profile_image_url", "profile_url")
df_tweets <- df_tweets[, col_order]
colnames(df_tweets)
col_order <- c("user_id", "status_id", "created_at","screen_name", "text",
"source", "conversation_id", "in_reply_to_user_id",
"mentions_screen_name","retweet_count","reply_count", "like_count",
"quote_count","hashtags","url_t.co", "url_extended_url","lang",
"geo_coords","place_id", "place_type","full_name", "country", "name.y" , "country_code" , "context_annotations","name","location", "description",
"followers_count","friends_count", "statuses_count",
"listed_count", "account_created_at","verified",
"profile_image_url", "profile_url")
df_tweets <- df_tweets[, col_order]
col_order
colnames(df_tweets)[colnames(df_tweets) == "name.y"] <- 'city_name'
colnames(df_tweets)[colnames(df_tweets) == "name.x"] <- 'name'
col_order <- c("user_id", "status_id", "created_at","screen_name", "text",
"source", "conversation_id", "in_reply_to_user_id",
"mentions_screen_name","retweet_count","reply_count", "like_count",
"quote_count","hashtags","url_t.co", "url_extended_url","lang",
"geo_coords","place_id", "place_type","full_name", "country", "city_name" , "country_code" , "context_annotations","name","location", "description",
"followers_count","friends_count", "statuses_count",
"listed_count", "account_created_at","verified",
"profile_image_url", "profile_url")
df_tweets <- df_tweets[, col_order]
df_tweets
get_tweet_preprocess <- function(list){
tweets_list_clean <- lapply(list, function(x){
##from x$data
selec_col <- c("created_at","id","text","lang","author_id", "source","conversation_id")
tmp <- x$data[selec_col]
for(i in 1:nrow(x$data)){
if("in_reply_to_user_id" %in% colnames(x$data) == TRUE){
in_reply_to_user_id <- x$data$in_reply_to_user_id
}else{
in_reply_to_user_id <- NA}
}
tmp$in_reply_to_user_id <- in_reply_to_user_id
tmp$geo_coords <- x$data$geo$coordinates$coordinates
tmp$place_id <- x$data$geo$place_id
tmp$context_annotations <- x$data$context_annotations
tmp <- cbind(tmp, x$data$public_metrics)
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_t.co <- x$data$entities$urls[[i]][[3]]
if(length(url_t.co)==0){
url_t.co <- NA
} else {
url_t.co <- url_t.co
}
tmp.1[[i]] <- url_t.co
}
tmp$url_t.co <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_extended_url<- x$data$entities$urls[[i]][[4]]
if(length(url_extended_url)==0){
url_extended_url <- NA
} else {
url_extended_url <- url_extended_url
}
tmp.1[[i]] <- url_extended_url
}
tmp$url_extended_url <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
hashtags <- x$data$entities$hashtags[[i]][[3]]
if(length(hashtags) == 0){
hashtags <- NA
} else{
hashtags <- hashtags
}
tmp.1[[i]] <- hashtags
}
tmp$hashtags <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
mentions_screen_name <- x$data$entities$mentions[[i]][[3]]
if(length(mentions_screen_name)==0){
mentions_screen_name <- NA
}  else {
mentions_screen_name <- mentions_screen_name
}
tmp.1[[i]] <- mentions_screen_name
}
tmp$mentions_screen_name <- tmp.1
## from user data
names(x$includes$users)[names(x$includes$users) == "id"] <- "author_id"
df <- left_join(x$data, x$includes$users, by = "author_id")
## follower count, following count, tweet count, listed_count
tmp$screen_name <-df$username
tmp <- cbind(tmp, df$public_metrics.y)
tmp$profile_image_url <-df$profile_image_url
tmp$name <-df$name
tmp$location <-df$location
tmp$description <-df$description
tmp$verified <-df$verified
tmp$profile_image_url <-df$profile_image_url
tmp$account_created_at <-df$created_at.y
tmp.2 <- list()
for (i in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[1]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.2[[i]] <- data.frame(profile_url)
}
tmp.2 <- do.call(rbind.data.frame, tmp.2)
tmp$profile_url <- tmp.2
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
tmp
})
df_tweets <- do.call(rbind.data.frame, tweets_list_clean)
colnames(df_tweets)[colnames(df_tweets) == 'id'] <- 'status_id'
colnames(df_tweets)[colnames(df_tweets) == "author_id"] <- 'user_id'
colnames(df_tweets)[colnames(df_tweets) == "following_count"] <- 'friends_count'
colnames(df_tweets)[colnames(df_tweets) == "tweet_count"] <- 'statuses_count'
colnames(df_tweets)[colnames(df_tweets) == "name.y"] <- 'city_name'
colnames(df_tweets)[colnames(df_tweets) == "name.x"] <- 'name'
geom <- list()
for(i in 1:nrow(df_tweets)){
if(length(df_tweets$geo_coords[[i]]) < 1){
df_tweets$geo_coords[[i]] <- c(NA, NA)
}
geom[[i]]<- c(df_tweets$geo_coords[[i]][[2]], df_tweets$geo_coords[[i]][[1]] )
}
df_tweets <-
df_tweets %>%
mutate(geo_coords = geom)
col_order <- c("user_id", "status_id", "created_at","screen_name", "text",
"source", "conversation_id", "in_reply_to_user_id",
"mentions_screen_name","retweet_count","reply_count", "like_count",
"quote_count","hashtags","url_t.co", "url_extended_url","lang",
"geo_coords","place_id", "place_type","full_name", "country", "city_name" , "country_code" , "context_annotations","name","location", "description",
"followers_count","friends_count", "statuses_count",
"listed_count", "account_created_at","verified",
"profile_image_url", "profile_url")
df_tweets <- df_tweets[, col_order]
df_tweets
return(df_tweets)
}
df_pre <- get_tweet_preprocess(list_tweets)
df_xxx <- geo_preprocess(df_pre, location_country = "spain",  multi_name = c("españa","spagna"))
load("F:/Bea/PostDoc/RESEARCH/ComTxt/data/list_tweets.rda")
library(udpipe)
library(dplyr)
library(purrr)
library(dplyr)
library(maps)
get_tweet_preprocess <- function(list){
tweets_list_clean <- lapply(list, function(x){
##from x$data
selec_col <- c("created_at","id","text","lang","author_id", "source","conversation_id")
tmp <- x$data[selec_col]
for(i in 1:nrow(x$data)){
if("in_reply_to_user_id" %in% colnames(x$data) == TRUE){
in_reply_to_user_id <- x$data$in_reply_to_user_id
}else{
in_reply_to_user_id <- NA}
}
tmp$in_reply_to_user_id <- in_reply_to_user_id
tmp$geo_coords <- x$data$geo$coordinates$coordinates
tmp$place_id <- x$data$geo$place_id
tmp$context_annotations <- x$data$context_annotations
tmp <- cbind(tmp, x$data$public_metrics)
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_t.co <- x$data$entities$urls[[i]][[3]]
if(length(url_t.co)==0){
url_t.co <- NA
} else {
url_t.co <- url_t.co
}
tmp.1[[i]] <- url_t.co
}
tmp$url_t.co <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_extended_url<- x$data$entities$urls[[i]][[4]]
if(length(url_extended_url)==0){
url_extended_url <- NA
} else {
url_extended_url <- url_extended_url
}
tmp.1[[i]] <- url_extended_url
}
tmp$url_extended_url <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
hashtags <- x$data$entities$hashtags[[i]][[3]]
if(length(hashtags) == 0){
hashtags <- NA
} else{
hashtags <- hashtags
}
tmp.1[[i]] <- hashtags
}
tmp$hashtags <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
mentions_screen_name <- x$data$entities$mentions[[i]][[3]]
if(length(mentions_screen_name)==0){
mentions_screen_name <- NA
}  else {
mentions_screen_name <- mentions_screen_name
}
tmp.1[[i]] <- mentions_screen_name
}
tmp$mentions_screen_name <- tmp.1
## from user data
names(x$includes$users)[names(x$includes$users) == "id"] <- "author_id"
df <- left_join(x$data, x$includes$users, by = "author_id")
## follower count, following count, tweet count, listed_count
tmp$screen_name <-df$username
tmp <- cbind(tmp, df$public_metrics.y)
tmp$profile_image_url <-df$profile_image_url
tmp$name <-df$name
tmp$location <-df$location
tmp$description <-df$description
tmp$verified <-df$verified
tmp$profile_image_url <-df$profile_image_url
tmp$account_created_at <-df$created_at.y
tmp.2 <- list()
for (i in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[1]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.2[[i]] <- data.frame(profile_url)
}
tmp.2 <- do.call(rbind.data.frame, tmp.2)
tmp$profile_url <- tmp.2
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
tmp
})
df_tweets <- do.call(rbind.data.frame, tweets_list_clean)
colnames(df_tweets)[colnames(df_tweets) == 'id'] <- 'status_id'
colnames(df_tweets)[colnames(df_tweets) == "author_id"] <- 'user_id'
colnames(df_tweets)[colnames(df_tweets) == "following_count"] <- 'friends_count'
colnames(df_tweets)[colnames(df_tweets) == "tweet_count"] <- 'statuses_count'
colnames(df_tweets)[colnames(df_tweets) == "name.y"] <- 'city_name'
colnames(df_tweets)[colnames(df_tweets) == "name.x"] <- 'name'
geom <- list()
for(i in 1:nrow(df_tweets)){
if(length(df_tweets$geo_coords[[i]]) < 1){
df_tweets$geo_coords[[i]] <- c(NA, NA)
}
geom[[i]]<- c(df_tweets$geo_coords[[i]][[2]], df_tweets$geo_coords[[i]][[1]] )
}
df_tweets <-
df_tweets %>%
mutate(geo_coords = geom)
col_order <- c("user_id", "status_id", "created_at","screen_name", "text",
"source", "conversation_id", "in_reply_to_user_id",
"mentions_screen_name","retweet_count","reply_count", "like_count",
"quote_count","hashtags","url_t.co", "url_extended_url","lang",
"geo_coords","place_id", "place_type","full_name", "country", "city_name" , "country_code" , "context_annotations","name","location", "description",
"followers_count","friends_count", "statuses_count",
"listed_count", "account_created_at","verified",
"profile_image_url", "profile_url")
df_tweets <- df_tweets[, col_order]
df_tweets
return(df_tweets)
}
df_pre <- get_tweet_preprocess(list_tweets)
library(purrr)
library(dplyr)
library(maps)
geo_preprocess <- function(df, location_country,  multi_name){
## Seperating city and country with ,
tmp <- list()
for(i in 1:length(df$location)){
tmp[[i]] <- unlist(strsplit(as.character(df$location[i]), ","))
}
location_1 <- list()
for(i in 1:length(tmp)){
location_1[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_1 <- reduce(location_1, bind_rows)
## location_tmp: seperating cities and country ()
tmp <- list()
for(i in 1:nrow(location_1)){
tmp[[i]] <- unlist(strsplit(as.character(location_1[i, 1]), "\\(|\\)"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_ df: combining location_1 data and location_tmp data adjusting contry coulmn
location_df <- cbind(location_tmp, location_1[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## location_tmp: seperating city and country with -
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[-]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_df: combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
##location_tmp: seperating city and country with _
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[_]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_df: combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## location_tmp : seperating cities "[.]"
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[.]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <-list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
##location_df :combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## cleaning text -------
location_df[,1] <- gsub("\\|..*", "", location_df[,1])##deleting multiple cities
location_df[,1] <- gsub("<u+.*", "", location_df[,1])##deleting unicode
location_df[,1] <- gsub("[0-9]+", "",location_df[,1], perl = T)
location_df[,2] <- gsub("[0-9]+", "",location_df[,2], perl = T)
location_df[,1] <- gsub("[[:punct:][:blank:]]+", " ", location_df[,1], perl = T)##deleting puntuation city
location_df[,2] <- gsub("[[:punct:][:blank:]]+", " ", location_df[,2], perl = T)##deleting puntuation country
location_df$country <- trimws(location_df$country)
##language specific
location_df[,2] <- tolower(location_df[,2])##lowercase
location_df[,1] <- tolower(location_df[,1])##lowercase
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$city %in% multi_name, location_country, location_df$country)
)
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$country %in% multi_name, location_country, location_df$country)
)
## if cities names is spain add to country column----
location_df <- as.data.frame(location_df)
location_df <-
location_df %>%
mutate(
country =ifelse(location_df[,1] == location_country, location_country, location_df[,2])
)
location_df[,1] <- gsub("^\\s+|\\s+$", "", location_df[,1])##deleting space
location_df[,2] <- gsub("^\\s+|\\s+$", "", location_df[,2])##deleting space
##geocode selecting
geo <- list()
for(i in 1:length(df$location)){
if(is.na(df$geo_coords[[i]][[1]])){
geo[[i]] <- list(lat = NA, lng = NA)
} else {
geo[[i]] <- list(lat = df$geo_coords[[i]][[1]], lng = df$geo_coords[[i]][[2]], country = location_country)
}
}
geo <- reduce(geo, bind_rows)
##location_df :combining loctaion_df and location tmp
location_df <- cbind(location_df, geo[,3])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
data("world.cities")
## now lets find the cities cities
world.cities$country.etc <- tolower(world.cities$country.etc)
city_df <- world.cities[world.cities$country.etc == location_country, ]
city_df$name <- tolower(city_df$name)
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$city %in% city_df$name, location_country, location_df$country)
)
## add to text_clean data fram
df <- subset(df, select = -c(location, country))
df <- data.frame(df, location_df)
## add lat lang from geo_coord
df <- cbind(df, geo[,1:2])
world.cities$country.etc <- tolower(world.cities$country.etc)
city_df <- world.cities[world.cities$country.etc == location_country, ]
city_df$name <- tolower(city_df$name)
#geom <- list()
for(i in 1:nrow(df)){
if(nrow(city_df[city_df$name == df$city[i],])==1){
df$lat[[i]] <- city_df[city_df$name == df$city[i],]$lat
df$lng[[i]] <- city_df[city_df$name == df$city[i],]$long
}
if(nrow(city_df[city_df$name == df$city[i],])==0) {
df$lat[[i]] <- NA
df$lng[[i]] <- NA
}
}
df <- df[which(df$country == location_country),]
return(df)
}
df_pre <- geo_preprocess(df_pre,location_country = "spain",  multi_name = c("españa","spagna" )
)
twitter_topic <- function(df, n_topic){
##load  data
df$status_id <- as.character(as.factor(df$status_id))
df <- df[, c("status_id", "text")]
colnames(df) <- c("doc.id", "text")
#df$text <- gsub("@\\w*", "", df$text)
#df$text <- gsub("#\\w*", "", df$text)
library(stringr)
for(i in 1:nrow(df)){
df$text[[i]] <- paste(str_extract_all(df$text[[i]], '\\w{4,}')[[1]], collapse=' ')
}
stop.tmp <- stopwords::data_stopwords_smart
#stopwords_text <- stopwords::stopwords(language, source = "stopwords-iso")
#stopwords_text <- c(stop.tmp, stopwords_text)
write.table(stop.tmp , file="stopwords.txt", fileEncoding="UTF-8", row.names = FALSE, col.names =  FALSE,quote = FALSE)
##mallet analysis
##import as mallet format
#mallet.instances <- mallet.import(df$doc.id, df$text, paste0("stopwords_", language, ".txt"), token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(df$doc.id, df$text, "stopwords.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
##mallet analysis
model <- MalletLDA(num.topics = as.numeric(n_topic))
model$model$setRandomSeed(12345L)
model$loadDocuments(mallet.instances)
## Optimize hyperparameters every 20 iterations, after 50 burn-in iteration
model$setAlphaOptimization(2000, 4000)
model$train(2000)
model$maximize(30)
mallet_df <- model
}
library(mallet)
library(stopwords)
df_topic <- twitter_topic(df_pre, n_topic = 7)
